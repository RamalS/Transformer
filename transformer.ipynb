{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pogledati kako normalization layer radi\n",
    "- dodati sentance embedding\n",
    "- runat na cudi\n",
    "- dodati loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiHadAttention\n",
    "\n",
    "Attention($Q$, $K$, $V$) = softmax($\\frac {QK^T} {\\sqrt {d_k}}$)$V$\n",
    "\n",
    "![attention](./attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # matmul Q and K and scale\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmax layer\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # matmul attention and V\n",
    "        context = torch.matmul(attention, V)\n",
    "\n",
    "        return context\n",
    "    \n",
    "    # dijeli x na batch_size, broj glava, duljinu rijeci i d_k\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, _ = x.size()\n",
    "\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # batch_size x seq_len x d_model\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V)\n",
    "\n",
    "        # splitaj Q, K i V na broj glava\n",
    "        # batch_size x num_heads x seq_len x d_k\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # vrati u oblik prije splitanja\n",
    "        # batch_size x seq_len x d_model\n",
    "        attention = self.combine_heads(attention)\n",
    "\n",
    "        output = self.output_linear(attention)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "\n",
    "$PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})$ za parne\n",
    "\n",
    "$PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})$ za neparne\n",
    "\n",
    "odnosno\n",
    "\n",
    "$PE_{(pos, i)} = sin(pos / 10000^{i / d_{model}})$ za parne\n",
    "\n",
    "$PE_{(pos, i+1)} = cos(pos / 10000^{i / d_{model}})$ za neparne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_pe = torch.sin(position / denominator)\n",
    "        odd_pe = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_pe, odd_pe], dim=2)\n",
    "        pe = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        \n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position wise feed forward network\n",
    "\n",
    "Sluzi za dodatno ucenje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) # d_model x d_ff\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x) # d_ff x d_model\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "![encoder](./encoder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # multi-head attention\n",
    "        attention = self.multi_head_attention(x, x, x, mask)\n",
    "\n",
    "        # add and norm\n",
    "        x = self.layer_norm1(x + attention)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # position-wise feed forward\n",
    "        feed_forward = self.position_wise_feed_forward(x)\n",
    "\n",
    "        # add and norm\n",
    "        x = self.layer_norm2(x + feed_forward)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "![decoder](./decoder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, trg_mask=None):\n",
    "        # masked multi-head attention\n",
    "        masked_attention = self.multi_head_attention1(x, x, x, trg_mask)\n",
    "\n",
    "        # add and norm\n",
    "        x = self.layer_norm1(x + masked_attention)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # multi-head attention\n",
    "        attention = self.multi_head_attention2(x, encoder_output, encoder_output, src_mask)\n",
    "\n",
    "        # add and norm\n",
    "        x = self.layer_norm2(x + attention)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # position-wise feed forward\n",
    "        feed_forward = self.position_wise_feed_forward(x)\n",
    "\n",
    "        # add and norm\n",
    "        x = self.layer_norm3(x + feed_forward)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_lenth, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_lenth)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # def generate_mask(self, src, tgt):\n",
    "    #     src_mask = (src != 0).unsqueeze(-2)\n",
    "\n",
    "    #     tgt_mask = (tgt != 0).unsqueeze(-2)\n",
    "    #     tgt_mask = tgt_mask & torch.tril(torch.ones(tgt.size(-1), tgt.size(-1))).type_as(tgt_mask).unsqueeze(0)\n",
    "\n",
    "    #     return src_mask, tgt_mask\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.linear(tgt)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_src_data = torch.randn(1, 100, 512)\n",
    "val_src_data.size()\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.703485488891602\n",
      "Epoch: 2, Loss: 8.652273178100586\n",
      "Epoch: 3, Loss: 8.610093116760254\n",
      "Epoch: 4, Loss: 8.581259727478027\n",
      "Epoch: 5, Loss: 8.549642562866211\n",
      "Epoch: 6, Loss: 8.524422645568848\n",
      "Epoch: 7, Loss: 8.500144004821777\n",
      "Epoch: 8, Loss: 8.48416519165039\n",
      "Epoch: 9, Loss: 8.46324348449707\n",
      "Epoch: 10, Loss: 8.438236236572266\n",
      "Epoch: 11, Loss: 8.420746803283691\n",
      "Epoch: 12, Loss: 8.39939022064209\n",
      "Epoch: 13, Loss: 8.377776145935059\n",
      "Epoch: 14, Loss: 8.369438171386719\n",
      "Epoch: 15, Loss: 8.344329833984375\n",
      "Epoch: 16, Loss: 8.33425235748291\n",
      "Epoch: 17, Loss: 8.312045097351074\n",
      "Epoch: 18, Loss: 8.300150871276855\n",
      "Epoch: 19, Loss: 8.26464557647705\n",
      "Epoch: 20, Loss: 8.256332397460938\n",
      "Epoch: 21, Loss: 8.234614372253418\n",
      "Epoch: 22, Loss: 8.219710350036621\n",
      "Epoch: 23, Loss: 8.200614929199219\n",
      "Epoch: 24, Loss: 8.188332557678223\n",
      "Epoch: 25, Loss: 8.170958518981934\n",
      "Epoch: 26, Loss: 8.14598274230957\n",
      "Epoch: 27, Loss: 8.135903358459473\n",
      "Epoch: 28, Loss: 8.123311042785645\n",
      "Epoch: 29, Loss: 8.1107816696167\n",
      "Epoch: 30, Loss: 8.094880104064941\n",
      "Epoch: 31, Loss: 8.07430362701416\n",
      "Epoch: 32, Loss: 8.06401538848877\n",
      "Epoch: 33, Loss: 8.047201156616211\n",
      "Epoch: 34, Loss: 8.029531478881836\n",
      "Epoch: 35, Loss: 8.012653350830078\n",
      "Epoch: 36, Loss: 7.996309757232666\n",
      "Epoch: 37, Loss: 7.967582702636719\n",
      "Epoch: 38, Loss: 7.958000183105469\n",
      "Epoch: 39, Loss: 7.93726921081543\n",
      "Epoch: 40, Loss: 7.91139030456543\n",
      "Epoch: 41, Loss: 7.909290790557861\n",
      "Epoch: 42, Loss: 7.871638774871826\n",
      "Epoch: 43, Loss: 7.85543155670166\n",
      "Epoch: 44, Loss: 7.832852840423584\n",
      "Epoch: 45, Loss: 7.820224285125732\n",
      "Epoch: 46, Loss: 7.798421859741211\n",
      "Epoch: 47, Loss: 7.7703537940979\n",
      "Epoch: 48, Loss: 7.745409965515137\n",
      "Epoch: 49, Loss: 7.729794979095459\n",
      "Epoch: 50, Loss: 7.708794593811035\n",
      "Epoch: 51, Loss: 7.671971321105957\n",
      "Epoch: 52, Loss: 7.651758670806885\n",
      "Epoch: 53, Loss: 7.631959438323975\n",
      "Epoch: 54, Loss: 7.6091413497924805\n",
      "Epoch: 55, Loss: 7.590063095092773\n",
      "Epoch: 56, Loss: 7.551450252532959\n",
      "Epoch: 57, Loss: 7.535829544067383\n",
      "Epoch: 58, Loss: 7.506300926208496\n",
      "Epoch: 59, Loss: 7.481404781341553\n",
      "Epoch: 60, Loss: 7.454658508300781\n",
      "Epoch: 61, Loss: 7.423592567443848\n",
      "Epoch: 62, Loss: 7.403819561004639\n",
      "Epoch: 63, Loss: 7.373000144958496\n",
      "Epoch: 64, Loss: 7.354788780212402\n",
      "Epoch: 65, Loss: 7.325556755065918\n",
      "Epoch: 66, Loss: 7.291597843170166\n",
      "Epoch: 67, Loss: 7.268764495849609\n",
      "Epoch: 68, Loss: 7.245823860168457\n",
      "Epoch: 69, Loss: 7.217306137084961\n",
      "Epoch: 70, Loss: 7.204237937927246\n",
      "Epoch: 71, Loss: 7.157485008239746\n",
      "Epoch: 72, Loss: 7.133247375488281\n",
      "Epoch: 73, Loss: 7.116141319274902\n",
      "Epoch: 74, Loss: 7.098160743713379\n",
      "Epoch: 75, Loss: 7.057575702667236\n",
      "Epoch: 76, Loss: 7.0438103675842285\n",
      "Epoch: 77, Loss: 7.014450550079346\n",
      "Epoch: 78, Loss: 6.990265846252441\n",
      "Epoch: 79, Loss: 6.963785648345947\n",
      "Epoch: 80, Loss: 6.939236164093018\n",
      "Epoch: 81, Loss: 6.908139705657959\n",
      "Epoch: 82, Loss: 6.886032581329346\n",
      "Epoch: 83, Loss: 6.865864276885986\n",
      "Epoch: 84, Loss: 6.833800792694092\n",
      "Epoch: 85, Loss: 6.811537742614746\n",
      "Epoch: 86, Loss: 6.780752658843994\n",
      "Epoch: 87, Loss: 6.7736663818359375\n",
      "Epoch: 88, Loss: 6.738527774810791\n",
      "Epoch: 89, Loss: 6.715809345245361\n",
      "Epoch: 90, Loss: 6.693972587585449\n",
      "Epoch: 91, Loss: 6.660558223724365\n",
      "Epoch: 92, Loss: 6.640439033508301\n",
      "Epoch: 93, Loss: 6.617958068847656\n",
      "Epoch: 94, Loss: 6.584163665771484\n",
      "Epoch: 95, Loss: 6.569498062133789\n",
      "Epoch: 96, Loss: 6.543936729431152\n",
      "Epoch: 97, Loss: 6.512759208679199\n",
      "Epoch: 98, Loss: 6.4950432777404785\n",
      "Epoch: 99, Loss: 6.472954750061035\n",
      "Epoch: 100, Loss: 6.449578285217285\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 100, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q = nn.Linear(512, 512)\n",
    "\n",
    "def split_heads(x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, 8, 64).transpose(1, 2)\n",
    "\n",
    "split_heads(W_Q(val_src_data)).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
